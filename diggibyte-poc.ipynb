{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93449889-de31-468e-8672-463cef97ebfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# This is in github\n",
    "### Databricks PySpark Notebook: Simple Medallion Architecture + Unity Catalog\n",
    "\n",
    "### Updated to use Unity Catalog directly (since Hive Metastore is disabled).\n",
    "_This simplified notebook shows the basics of:_\n",
    "- Bronze → Silver → Gold layers (medallion architecture)\n",
    "- explode + aggregation\n",
    "- Read/write to Catalog schema\n",
    "- Performance tips (broadcast join, partitioning, Z-ORDER, query comparison)\n",
    "- Short interview notes as comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667a8fa7-9ab1-4db8-b651-c4224e8e6752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SETUP (Unity Catalog)\n",
    "# -------------------------------------------------------------------\n",
    "CATALOG = \"diggibyte\"  # catalog name\n",
    "SCHEMA = \"poc\"\n",
    "\n",
    "# Create the catalog if it does not exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BRONZE: Raw ingest using simplified JSON\n",
    "# -------------------------------------------------------------------\n",
    "bronze_data = [\n",
    "    (\n",
    "        1001,\n",
    "        \"AR-001\",\n",
    "        \"2025-08-01T10:03:22Z\",\n",
    "        '[{\"ar\":\"SAR-1\",\"qty\":2,\"price\":19.99},'\n",
    "        ' {\"ar\":\"SAR-2\",\"qty\":1,\"price\":5.25}]',\n",
    "        \"WELCOME10;FREESHIP\",\n",
    "        \"Bengaluru\",\n",
    "        \"IN\",\n",
    "    ),\n",
    "    (\n",
    "        1002,\n",
    "        \"AR-002\",\n",
    "        \"2025-08-01T11:15:05Z\",\n",
    "        '[{\"ar\":\"SAR-2\",\"qty\":4,\"price\":5.00},'\n",
    "        ' {\"ar\":\"SAR-3\",\"qty\":1,\"price\":99.00}]',\n",
    "        \"LOYAL5\",\n",
    "        \"Mumbai\",\n",
    "        \"IN\",\n",
    "    ),\n",
    "    (\n",
    "        1003,\n",
    "        \"AR-001\",\n",
    "        \"2025-08-02T09:01:11Z\",\n",
    "        '[{\"ar\":\"SAR-1\",\"qty\":1,\"price\":19.99},'\n",
    "        ' {\"ar\":\"SAR-4\",\"qty\":6,\"price\":2.00}]',\n",
    "        \"\",\n",
    "        \"Delhi\",\n",
    "        \"IN\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "schema = \"order_id INT, customer_id STRING, order_ts STRING, items_json STRING, promotions STRING, shipping_city STRING, shipping_country STRING\"\n",
    "bronze_df = spark.createDataFrame(bronze_data, schema)\n",
    "bronze_tbl = f\"{CATALOG}.{SCHEMA}.bronze_orders\"\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_tbl)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SILVER: Parse + explode items and promotions\n",
    "# -------------------------------------------------------------------\n",
    "item_schema = T.ArrayType(\n",
    "    T.StructType(\n",
    "        [\n",
    "            T.StructField(\"ar\", T.StringType()),\n",
    "            T.StructField(\"qty\", T.IntegerType()),\n",
    "            T.StructField(\"price\", T.DoubleType()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "b = spark.table(bronze_tbl)\n",
    "silver_df = (\n",
    "    b.withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
    "    .withColumn(\"items\", F.from_json(\"items_json\", item_schema))\n",
    "    .withColumn(\"item\", F.explode(\"items\"))\n",
    "    # Fix: keep non-promoted items with NULL promotion instead of dropping them\n",
    "    .withColumn(\n",
    "        \"promotions_arr\",\n",
    "        F.when(F.length(\"promotions\") > 0, F.split(\"promotions\", \";\")).otherwise(\n",
    "            F.array(F.lit(None).cast(\"string\"))\n",
    "        ),\n",
    "    )\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_ts\",\n",
    "        \"shipping_city\",\n",
    "        \"shipping_country\",\n",
    "        F.col(\"item.ar\").alias(\"ar\"),\n",
    "        F.col(\"item.qty\").alias(\"qty\"),\n",
    "        F.col(\"item.price\").alias(\"price\"),\n",
    "        (F.col(\"item.qty\") * F.col(\"item.price\")).alias(\"item_total\"),\n",
    "        F.explode_outer(\"promotions_arr\").alias(\"promotion\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "silver_tbl = f\"{CATALOG}.{SCHEMA}.silver_order_items\"\n",
    "(\n",
    "    silver_df.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"shipping_country\")  # Partition by country for scale\n",
    "    .saveAsTable(silver_tbl)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GOLD: Aggregations\n",
    "# -------------------------------------------------------------------\n",
    "# Customer-level revenue\n",
    "customer_gold_df = silver_df.groupBy(\"customer_id\").agg(\n",
    "    F.sum(\"item_total\").alias(\"total_spent\")\n",
    ")\n",
    "\n",
    "customer_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_customer_revenue\"\n",
    "(\n",
    "    customer_gold_df.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(customer_gold_tbl)\n",
    ")\n",
    "spark.sql(f\"OPTIMIZE {customer_gold_tbl} ZORDER BY (customer_id)\")\n",
    "\n",
    "# Promotion-level revenue\n",
    "promotion_gold_df = (\n",
    "    silver_df.groupBy(\"promotion\")\n",
    "    .agg(F.sum(\"item_total\").alias(\"total_spent\"))\n",
    "    .filter(F.col(\"promotion\").isNotNull())\n",
    ")\n",
    "\n",
    "promotion_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_promotion_revenue\"\n",
    "(\n",
    "    promotion_gold_df.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(promotion_gold_tbl)\n",
    ")\n",
    "spark.sql(f\"OPTIMIZE {promotion_gold_tbl} ZORDER BY (promotion)\")\n",
    "\n",
    "# City-level revenue\n",
    "city_gold_df = silver_df.groupBy(\"shipping_city\").agg(\n",
    "    F.sum(\"item_total\").alias(\"total_spent\")\n",
    ")\n",
    "\n",
    "city_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_city_revenue\"\n",
    "(city_gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(city_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {city_gold_tbl} ZORDER BY (shipping_city)\")\n",
    "\n",
    "# Daily revenue (time-based aggregation)\n",
    "daily_gold_df = silver_df.groupBy(F.to_date(\"order_ts\").alias(\"order_date\")).agg(\n",
    "    F.sum(\"item_total\").alias(\"daily_revenue\")\n",
    ")\n",
    "\n",
    "daily_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_daily_revenue\"\n",
    "(daily_gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(daily_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {daily_gold_tbl} ZORDER BY (order_date)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PERFORMANCE TIP (broadcast join)\n",
    "# -------------------------------------------------------------------\n",
    "small_dim = spark.createDataFrame(\n",
    "    [\n",
    "        (\"SAR-1\", \"Clothes\"),\n",
    "        (\"SAR-2\", \"Shoes\"),\n",
    "        (\"SAR-3\", \"Electronics\"),\n",
    "        (\"SAR-4\", \"Snacks\"),\n",
    "    ],\n",
    "    [\"ar\", \"category\"],\n",
    ")\n",
    "joined = silver_df.join(F.broadcast(small_dim), \"ar\", \"left\")\n",
    "joined.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PERFORMANCE COMPARISON: With vs Without Z-ORDER\n",
    "# -------------------------------------------------------------------\n",
    "import time\n",
    "\n",
    "# Without Z-ORDER (raw filter scan)\n",
    "start = time.time()\n",
    "spark.sql(f\"SELECT * FROM {customer_gold_tbl} WHERE customer_id = 'AR-001'\").collect()\n",
    "print(\"Query time without Z-ORDER:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# With Z-ORDER (optimized table)\n",
    "start = time.time()\n",
    "spark.sql(f\"SELECT * FROM {customer_gold_tbl} WHERE customer_id = 'AR-001'\").collect()\n",
    "print(\"Query time with Z-ORDER:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d985882-ed45-4e3c-9c09-9e5e65ed67bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sql to list table contents\n",
    "%sql\n",
    "Select * from diggibyte.poc.bronze_orders;\n",
    "select * from diggibyte.poc.gold_city_revenue;\n",
    "select * from diggibyte.poc.gold_customer_revenue;\n",
    "select * from diggibyte.poc.gold_daily_revenue;\n",
    "select * from diggibyte.poc.gold_promotion_revenue;\n",
    "select * from diggibyte.poc.silver_order_items;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29eaf524-1ad4-4282-975c-e3b01c8e13b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DataFrame API to list the table records\n",
    "df = spark.table(\"diggibyte.poc.bronze_orders\")\n",
    "df.show()\n",
    "\n",
    "df = spark.table(\"diggibyte.poc.gold_city_revenue\")\n",
    "df.show()\n",
    "\n",
    "df = spark.table(\"diggibyte.poc.gold_customer_revenue\")\n",
    "df.show()\n",
    "\n",
    "df = spark.table(\"diggibyte.poc.gold_daily_revenue\")\n",
    "df.show()\n",
    "\n",
    "df = spark.table(\"diggibyte.poc.gold_promotion_revenue\")\n",
    "df.show()\n",
    "\n",
    "df = spark.table(\"diggibyte.poc.silver_order_items\")\n",
    "df.show()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5845900806511471,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "diggibyte-poc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
