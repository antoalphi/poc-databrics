{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "667a8fa7-9ab1-4db8-b651-c4224e8e6752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks PySpark Notebook: Simple Medallion Architecture + Unity Catalog\n",
    "# -------------------------------------------------------------------\n",
    "# Updated to use Unity Catalog directly (since Hive Metastore is disabled).\n",
    "# This simplified notebook shows the basics of:\n",
    "# 1. Bronze → Silver → Gold layers (medallion architecture)\n",
    "# 2. explode + aggregation\n",
    "# 3. Read/write to Catalog schema\n",
    "# 4. Performance tips (broadcast join, partitioning, Z-ORDER, query comparison)\n",
    "# 5. Short interview notes as comments\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SETUP (Unity Catalog)\n",
    "# -------------------------------------------------------------------\n",
    "CATALOG = \"diggibyte\"   # replace with the catalog name you have access to\n",
    "SCHEMA = \"poc\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BRONZE: Raw ingest using simplified JSON\n",
    "# -------------------------------------------------------------------\n",
    "bronze_data = [\n",
    "    (1001, \"C-001\", \"2025-08-01T10:03:22Z\",\n",
    "     '[{\"sku\":\"SKU-1\",\"qty\":2,\"price\":19.99},'\n",
    "     ' {\"sku\":\"SKU-2\",\"qty\":1,\"price\":5.25}]',\n",
    "     \"WELCOME10;FREESHIP\", \"Bengaluru\", \"IN\"),\n",
    "\n",
    "    (1002, \"C-002\", \"2025-08-01T11:15:05Z\",\n",
    "     '[{\"sku\":\"SKU-2\",\"qty\":4,\"price\":5.00},'\n",
    "     ' {\"sku\":\"SKU-3\",\"qty\":1,\"price\":99.00}]',\n",
    "     \"LOYAL5\", \"Mumbai\", \"IN\"),\n",
    "\n",
    "    (1003, \"C-001\", \"2025-08-02T09:01:11Z\",\n",
    "     '[{\"sku\":\"SKU-1\",\"qty\":1,\"price\":19.99},'\n",
    "     ' {\"sku\":\"SKU-4\",\"qty\":6,\"price\":2.00}]',\n",
    "     \"\", \"Delhi\", \"IN\")\n",
    "]\n",
    "\n",
    "schema = \"order_id INT, customer_id STRING, order_ts STRING, items_json STRING, promotions STRING, shipping_city STRING, shipping_country STRING\"\n",
    "bronze_df = spark.createDataFrame(bronze_data, schema)\n",
    "bronze_tbl = f\"{CATALOG}.{SCHEMA}.bronze_orders\"\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_tbl)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SILVER: Parse + explode items and promotions\n",
    "# -------------------------------------------------------------------\n",
    "item_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"sku\", T.StringType()),\n",
    "    T.StructField(\"qty\", T.IntegerType()),\n",
    "    T.StructField(\"price\", T.DoubleType())\n",
    "]))\n",
    "\n",
    "b = spark.table(bronze_tbl)\n",
    "silver_df = (b\n",
    "    .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
    "    .withColumn(\"items\", F.from_json(\"items_json\", item_schema))\n",
    "    .withColumn(\"item\", F.explode(\"items\"))\n",
    "    # Fix: keep non-promoted items with NULL promotion instead of dropping them\n",
    "    .withColumn(\"promotions_arr\", F.when(F.length(\"promotions\")>0, F.split(\"promotions\", \";\"))\n",
    "                                   .otherwise(F.array(F.lit(None).cast(\"string\"))))\n",
    "    .select(\"order_id\", \"customer_id\", \"order_ts\", \"shipping_city\", \"shipping_country\",\n",
    "            F.col(\"item.sku\").alias(\"sku\"),\n",
    "            F.col(\"item.qty\").alias(\"qty\"),\n",
    "            F.col(\"item.price\").alias(\"price\"),\n",
    "            (F.col(\"item.qty\")*F.col(\"item.price\")).alias(\"item_total\"),\n",
    "            F.explode_outer(\"promotions_arr\").alias(\"promotion\"))\n",
    ")\n",
    "\n",
    "silver_tbl = f\"{CATALOG}.{SCHEMA}.silver_order_items\"\n",
    "(silver_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"shipping_country\")  # Partition by country for scale\n",
    "    .saveAsTable(silver_tbl))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GOLD: Aggregations\n",
    "# -------------------------------------------------------------------\n",
    "# Customer-level revenue\n",
    "customer_gold_df = (silver_df\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(F.sum(\"item_total\").alias(\"total_spent\"))\n",
    ")\n",
    "\n",
    "customer_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_customer_revenue\"\n",
    "(customer_gold_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(customer_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {customer_gold_tbl} ZORDER BY (customer_id)\")\n",
    "\n",
    "# Promotion-level revenue\n",
    "promotion_gold_df = (silver_df\n",
    "    .groupBy(\"promotion\")\n",
    "    .agg(F.sum(\"item_total\").alias(\"total_spent\"))\n",
    "    .filter(F.col(\"promotion\").isNotNull())\n",
    ")\n",
    "\n",
    "promotion_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_promotion_revenue\"\n",
    "(promotion_gold_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(promotion_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {promotion_gold_tbl} ZORDER BY (promotion)\")\n",
    "\n",
    "# City-level revenue\n",
    "city_gold_df = (silver_df\n",
    "    .groupBy(\"shipping_city\")\n",
    "    .agg(F.sum(\"item_total\").alias(\"total_spent\"))\n",
    ")\n",
    "\n",
    "city_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_city_revenue\"\n",
    "(city_gold_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(city_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {city_gold_tbl} ZORDER BY (shipping_city)\")\n",
    "\n",
    "# Daily revenue (time-based aggregation)\n",
    "daily_gold_df = (silver_df\n",
    "    .groupBy(F.to_date(\"order_ts\").alias(\"order_date\"))\n",
    "    .agg(F.sum(\"item_total\").alias(\"daily_revenue\"))\n",
    ")\n",
    "\n",
    "daily_gold_tbl = f\"{CATALOG}.{SCHEMA}.gold_daily_revenue\"\n",
    "(daily_gold_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(daily_gold_tbl))\n",
    "spark.sql(f\"OPTIMIZE {daily_gold_tbl} ZORDER BY (order_date)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PERFORMANCE TIP (broadcast join)\n",
    "# -------------------------------------------------------------------\n",
    "small_dim = spark.createDataFrame([(\"SKU-1\",\"Clothes\"),(\"SKU-2\",\"Shoes\"),(\"SKU-3\",\"Electronics\"),(\"SKU-4\",\"Snacks\")],[\"sku\",\"category\"])\n",
    "joined = silver_df.join(F.broadcast(small_dim), \"sku\", \"left\")\n",
    "joined.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PERFORMANCE COMPARISON: With vs Without Z-ORDER\n",
    "# -------------------------------------------------------------------\n",
    "import time\n",
    "\n",
    "# Without Z-ORDER (raw filter scan)\n",
    "start = time.time()\n",
    "spark.sql(f\"SELECT * FROM {customer_gold_tbl} WHERE customer_id = 'C-001'\").collect()\n",
    "print(\"Query time without Z-ORDER:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# With Z-ORDER (optimized table)\n",
    "start = time.time()\n",
    "spark.sql(f\"SELECT * FROM {customer_gold_tbl} WHERE customer_id = 'C-001'\").collect()\n",
    "print(\"Query time with Z-ORDER:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# INTERVIEW NOTES (in comments)\n",
    "# -------------------------------------------------------------------\n",
    "# - Bronze: raw, append-only\n",
    "# - Silver: cleaned, normalized (explode, types)\n",
    "# - Gold: aggregated, BI-ready\n",
    "#   - Customer revenue: spend per customer\n",
    "#   - Promotion revenue: spend per promotion code\n",
    "#   - City revenue: spend per city (geo insight)\n",
    "#   - Daily revenue: spend per date (temporal insight)\n",
    "# - Explode: normalizes nested arrays into rows (items, promotions)\n",
    "# - Aggregations: groupBy + sum, avg, etc.\n",
    "# - Catalog: use fully qualified names (catalog.schema.table) with Unity Catalog\n",
    "# - Performance:\n",
    "#   - broadcast small tables\n",
    "#   - partitionBy on write for large datasets\n",
    "#   - OPTIMIZE with ZORDER on high-cardinality query columns\n",
    "#   - Benchmark queries before/after OPTIMIZE to demonstrate improvements\n",
    "# - Delta: ACID, schema evolution, time travel\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "diggibyte-poc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
