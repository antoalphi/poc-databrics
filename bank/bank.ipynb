{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a96130-4332-43d3-8c7c-f7ab36217934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS bank;\n",
    "CREATE SCHEMA IF NOT EXISTS bank.customer;\n",
    "CREATE VOLUME IF NOT EXISTS bank.customer.raw;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64c30f0-0133-4710-95ca-803d1f916501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "\n",
    "def send_email(subject, body):\n",
    "    sender = \"antoalphi.1@gmail.com\"\n",
    "    recipient = \"antoalphi.1@gmail.com\"\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender\n",
    "    msg[\"To\"] = recipient\n",
    "\n",
    "    try:\n",
    "        smtp = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "        smtp.starttls()\n",
    "        smtp.login(\"antoalphi.1@gmail.com\", \"uann hayy ftwd erfx\")\n",
    "        smtp.sendmail(sender, [recipient], msg.as_string())\n",
    "        smtp.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40be492-b15e-4d68-830b-3e13d537ce51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from datetime import datetime\n",
    "\n",
    "# Define volume path\n",
    "volume_path = \"/Volumes/bank/customer/raw/\"\n",
    "\n",
    "# Generate dynamic batch ID based on current date\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "batch_id = f\"batch_{current_date}\"\n",
    "\n",
    "\n",
    "# List files in the volume\n",
    "try:\n",
    "    files = dbutils.fs.ls(volume_path)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing file path: {str(e)}\"\n",
    "    send_email(f\"Failed to list files in volume: error_message\", error_message)\n",
    "    raise\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = file.path\n",
    "    file_name = os.path.basename(file_path).split(\".\")[0]\n",
    "    table_name = f\"bank.customer.{file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Detect file format and read accordingly\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_path.endswith(\".json\"):\n",
    "            df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "        elif file_path.endswith(\".parquet\"):\n",
    "            df = spark.read.parquet(file_path)\n",
    "        else:\n",
    "            send_email(f\"Unsupported file format: {file_path}\", \"Unsupported file format\")\n",
    "            continue\n",
    "\n",
    "        # Add metadata columns\n",
    "        df = (\n",
    "            df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_file_name\", lit(file_name))\n",
    "            .withColumn(\"source_path\", lit(file_path))\n",
    "            .withColumn(\"ingested_by\", lit(\"databricks_job\"))\n",
    "            .withColumn(\"batch_id\", lit(batch_id))\n",
    "        )\n",
    "\n",
    "        # Write to Delta table with partitioning if applicable\n",
    "        # Partition by ingestion date (not business fields)\n",
    "        if \"Geography\" in df.columns:\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"region\").saveAsTable(table_name)\n",
    "        else:\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n",
    "\n",
    "        send_email(\"Job Success\", \"✅ Successfully ingested and created table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing file {file_path}: {str(e)}\"\n",
    "        send_email(\" ❌ Databricks Job Failure\", error_message)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7364412019189188,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bank",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
